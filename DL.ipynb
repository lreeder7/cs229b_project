{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmsEt10pic7N"
      },
      "source": [
        "# CS 229B Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEbftpThVl-T"
      },
      "source": [
        "Resource that explains time series forecasting with RNNs (will explain how to get X,y, etc.) :\n",
        "https://www.geeksforgeeks.org/time-series-forecasting-using-recurrent-neural-networks-rnn-in-tensorflow/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "094MANVaMcwh"
      },
      "source": [
        "## Upload Dataset & Install packages\n",
        "\n",
        "The FS Peptide dataset contains 28 trajectories, each with 10000 frames.\n",
        "\n",
        "We need to install packages for data preprocessing (MDTraj) and for the models we are running (PyEMMA, PyTorch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZnWQ14URisX"
      },
      "outputs": [],
      "source": [
        "!wget https://ndownloader.figshare.com/articles/1030363/versions/1 -O fs_peptide.zip\n",
        "!unzip -o fs_peptide.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5TfIPqqS0Hs"
      },
      "outputs": [],
      "source": [
        "# !pip install --pre torch\n",
        "!pip install torch\n",
        "!pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install xformers\n",
        "!pip install mdtraj pyemma\n",
        "!pip install mdshare\n",
        "!pip install torchprofile\n",
        "!pip install deepspeed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCKSrYhjjEdl"
      },
      "source": [
        "Load dataset into files (including corresponding pdb file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX6tbj4aoXd_"
      },
      "source": [
        "# Preprocessing Data\n",
        "Please run the dataloader prior to running any of the following models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyPoFa5ZPRs1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import warnings\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "import pyemma\n",
        "from pyemma.util.contexts import settings\n",
        "import mdtraj as md\n",
        "import mdshare\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, pdb, files, chunk_size = 1000, mode='chunk'):\n",
        "        self.pdb = pdb\n",
        "        self.files = files\n",
        "        with warnings.catch_warnings():\n",
        "          warnings.simplefilter(\"ignore\")\n",
        "          self.topology = md.load(pdb).topology\n",
        "          self.ref_traj = md.load_xtc(files[0], top=pdb)\n",
        "        self.chunk_size = chunk_size\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "\n",
        "    def set_mode(self, mode):\n",
        "        self.mode = mode\n",
        "        print(\"updated mode to: \", mode)\n",
        "\n",
        "\n",
        "    def set_chunk_size(self, chunk_size):\n",
        "        self.chunk_size = chunk_size\n",
        "        print(\"updated chunk_size to: \", chunk_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert self.mode in ['chunk', 'truncate']\n",
        "        with warnings.catch_warnings():\n",
        "          warnings.simplefilter(\"ignore\")\n",
        "          traj = md.load_xtc(self.files[idx], top=self.pdb)\n",
        "\n",
        "        traj.superpose(self.ref_traj, frame=0)\n",
        "        traj.center_coordinates()\n",
        "\n",
        "        backbone = traj.atom_slice(self.topology.select(\"protein and backbone\"))\n",
        "        coords = backbone.xyz\n",
        "\n",
        "        traj_coords = coords.reshape(-1, backbone.n_atoms*3)\n",
        "\n",
        "        traj_coords = torch.from_numpy(traj_coords) # shape 264 x 10000 --> 10000 x 264\n",
        "\n",
        "        if self.mode == 'chunk':\n",
        "            traj_coords = traj_coords.reshape(-1,self.chunk_size, 264)\n",
        "        else:\n",
        "            traj_coords = traj_coords[:self.chunk_size, :].unsqueeze(0)\n",
        "        X = traj_coords[:,:-1, :]\n",
        "        Y = traj_coords[:,1:, :]\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "def load_files(source = '.'):\n",
        "    files = [Path(source).resolve() / f'trajectory-{i}.xtc' for i in range(1,29)]\n",
        "    pdb = '100-fs-peptide-400K.pdb'\n",
        "    return files, pdb\n",
        "\n",
        "def build_loaders(source = '.',\n",
        "                  train_batch_size = 4,\n",
        "                  test_batch_size = 8,\n",
        "                  num_workers = 2,\n",
        "                  seed = 1, test_frac = 0.2,train_drop = 0):\n",
        "    files, pdb = load_files(source)\n",
        "    np.random.seed(seed)\n",
        "    train_files = np.random.choice(files, size=int(len(files)* (1 - test_frac)), replace=False)\n",
        "\n",
        "    test_files = np.setdiff1d(files, train_files)\n",
        "    train_files = np.random.choice(train_files,size=int(len(train_files)* (1 - train_drop)), replace=False)\n",
        "\n",
        "    train_dataset = CustomDataset(pdb, train_files)\n",
        "    test_dataset = CustomDataset(pdb, test_files)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=num_workers)\n",
        "    print(f'Number of training samples: {len(train_dataset)}')\n",
        "    print(f'Number of testing samples: {len(test_dataset)}')\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def save_pkl(filename, save_object):\n",
        "    writer = open(filename,'wb')\n",
        "    pickle.dump(save_object, writer)\n",
        "    writer.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dnwc0frCYoo"
      },
      "outputs": [],
      "source": [
        "INPUT_SIZE = 264\n",
        "OUTPUT_SIZE = 264\n",
        "SEQ_LEN = 999\n",
        "\n",
        "# Define an RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size,\n",
        "                 nonlinearity='tanh', dropout=0.):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers,\n",
        "                          batch_first=True, nonlinearity=nonlinearity,\n",
        "                          dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def init_hidden(self, device, batch_size=1):\n",
        "        return torch.zeros(batch_size, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden = None,return_hidden = False):\n",
        "        B, T, d = x.size()\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(x.device, batch_size=B)\n",
        "        out, h = self.rnn(x)\n",
        "        out = self.fc(out)\n",
        "        if return_hidden:\n",
        "            return out, h\n",
        "        return out\n",
        "\n",
        "    def forward_autoregressive(self, x0, max_len, hidden = None):\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(x0.device, batch_size=x0.size(0))\n",
        "\n",
        "        all_out = torch.zeros(x0.size(0), max_len+1, x0.size(2)).to(x0.device)\n",
        "        all_out[:,0,:] = x0[:,0,:]\n",
        "        xt = x0\n",
        "        for t in range(max_len):\n",
        "            out, hidden = self.forward(xt,hidden = hidden,return_hidden = True)\n",
        "            all_out[:,t+1,:] = out.squeeze(1)\n",
        "            xt = out\n",
        "\n",
        "        return all_out[:, 1:, :]\n",
        "\n",
        "import xformers\n",
        "from xformers.components.feedforward import MLP\n",
        "from xformers.components.attention import NystromAttention\n",
        "\n",
        "class XformerBlock(nn.Module):\n",
        "    def __init__(self, num_heads = 8,\n",
        "                 embed_size = 32,\n",
        "                 num_landmarks = 64, causal = True):\n",
        "        super().__init__()\n",
        "        self.attn = NystromAttention(\n",
        "            dropout = 0.,\n",
        "            num_heads = num_heads,\n",
        "            num_landmarks = num_landmarks,\n",
        "            causal = causal\n",
        "        )\n",
        "        self.ff = MLP(dim_model=embed_size,\n",
        "          hidden_layer_multiplier=4,\n",
        "          activation=\"gelu\",\n",
        "          dropout=0.)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attn(k=x, q=x, v=x) + x\n",
        "        x = self.norm1(x)\n",
        "        x = self.ff(x) + x\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, emb_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * (-math.log(10000.0) / emb_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "# Implement causal masking\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "    return mask\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, num_heads, num_layers, output_size,seq_len):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_size, embed_size)\n",
        "        self.pos_encoder = PositionalEncoding(embed_size, max_len=SEQ_LEN)\n",
        "        self.max_len = SEQ_LEN\n",
        "        self.embed_size = embed_size\n",
        "        transformer_layers = []\n",
        "        for _ in range(num_layers):\n",
        "            transformer_layers.append(XformerBlock(num_heads, embed_size, num_landmarks = 64, causal = True))\n",
        "        self.transformer = nn.Sequential(*transformer_layers)\n",
        "        self.fc = nn.Linear(embed_size, output_size)\n",
        "\n",
        "    def adjust_pe(self, new_seq_len):\n",
        "        if self.max_len < new_seq_len:\n",
        "            self.pos_encoder = PositionalEncoding(self.embed_size, max_len=new_seq_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, d = x.size()\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward_autoregressive(self, x0, max_len):\n",
        "        all_out = torch.zeros(x0.size(0), max_len+1, x0.size(2)).to(x0.device) # preallocate output\n",
        "        all_out[:,0,:] = x0[:,0,:]\n",
        "        for t in tqdm(range(max_len)):\n",
        "            xt = all_out[:, :t+1, :]\n",
        "            next_output = self.forward(xt)[:,-1,:].unsqueeze(1)\n",
        "            all_out[:,t+1,:] = next_output.squeeze(1)\n",
        "        return all_out[:, 1:, :]\n",
        "\n",
        "\n",
        "\n",
        "# Implement Transformer Native Model\n",
        "class TransformerModel_NN(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, num_heads, num_layers, output_size,seq_len):\n",
        "        super(TransformerModel_NN, self).__init__()\n",
        "        self.embedding = nn.Linear(input_size, embed_size)\n",
        "        self.pos_encoder = PositionalEncoding(embed_size, max_len=SEQ_LEN)\n",
        "        self.max_len = SEQ_LEN\n",
        "        self.embed_size = embed_size\n",
        "        transformer_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, batch_first=True,\n",
        "                                                       dim_feedforward = 4 * embed_size, dropout = 0)\n",
        "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(embed_size, output_size)\n",
        "        self.register_buffer(\"mask\", generate_square_subsequent_mask(100))\n",
        "\n",
        "    def adjust_pe(self, new_seq_len):\n",
        "        if self.max_len < new_seq_len:\n",
        "            self.pos_encoder = PositionalEncoding(self.embed_size, max_len=new_seq_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, d = x.size()\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        if self.mask.size(0) < T:\n",
        "            self.register_buffer(\"mask\", generate_square_subsequent_mask(T).to(x.device))\n",
        "        mask = self.mask[:T, :T]\n",
        "\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer(x, mask=mask)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward_autoregressive(self, x0, max_len):\n",
        "\n",
        "        all_out = torch.zeros(x0.size(0), max_len+1, x0.size(2)).to(x0.device) # preallocate output\n",
        "        all_out[:,0,:] = x0[:,0,:]\n",
        "        for t in tqdm(range(max_len)):\n",
        "            xt = all_out[:, :t+1, :]\n",
        "            next_output = self.forward(xt)[:,-1,:].unsqueeze(1)\n",
        "            all_out[:,t+1,:] = next_output.squeeze(1)\n",
        "        return all_out[:, 1:, :]\n",
        "\n",
        "from torch.nn.utils.parametrizations import weight_norm\n",
        "# Apply convolution with causal padding: following https://discuss.pytorch.org/t/causal-convolution/3456/4\n",
        "class CausalConv1d(nn.Conv1d):\n",
        "    def __init__(self, input_size, output_size, kernel_size, stride=1, dilation=1):\n",
        "        padding = (kernel_size - 1) * dilation\n",
        "        super(CausalConv1d, self).__init__(input_size, output_size, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super(CausalConv1d, self).forward(x)\n",
        "        if self.padding[0] != 0:\n",
        "            return x[:, :, :-self.padding[0]]\n",
        "        return x\n",
        "\n",
        "class ResidTempBlock(nn.Module):\n",
        "    def __init__(self, input_size, channel_size, output_size, kernel_size, stride=1, dilation=1, dropout=0):\n",
        "        super(ResidTempBlock, self).__init__()\n",
        "\n",
        "        # do causal convolution here\n",
        "        self.conv1 = weight_norm(CausalConv1d(input_size,\n",
        "                                                             channel_size,\n",
        "                                                             kernel_size,\n",
        "                                                             stride=stride,\n",
        "                                                             dilation=dilation))\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        # do causal convolution here\n",
        "        self.conv2 = weight_norm(CausalConv1d(channel_size,\n",
        "                                                             output_size,\n",
        "                                                             kernel_size,\n",
        "                                                             stride=stride,\n",
        "                                                             dilation=dilation))\n",
        "\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.relu1, self.dropout1,\n",
        "                                     self.conv2, self.relu2, self.dropout2)\n",
        "\n",
        "        self.reshape = nn.Conv1d(input_size, output_size, 1) if input_size != output_size else None\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv2.weight.data.normal_(0, 0.01)\n",
        "        if self.reshape is not None:\n",
        "          self.reshape.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        residual = x if self.reshape is None else self.reshape(x)\n",
        "        out = self.relu(out + residual)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TCNModel(nn.Module):\n",
        "    def __init__(self, input_size, channel_size, input_length,\n",
        "                 kernel_size, stride=1, dropout=0):\n",
        "        super(TCNModel, self).__init__()\n",
        "\n",
        "        self.input_length = input_length\n",
        "        self.input_size = input_size\n",
        "        num_layers = len(channel_size)\n",
        "\n",
        "        layers=[]\n",
        "        for i in range(num_layers):\n",
        "            dilation_size = 2**i\n",
        "            in_channels = input_size if i == 0 else channel_size[i-1]\n",
        "            out_channels = input_size if i == (num_layers-1) else channel_size[i]\n",
        "\n",
        "            layers.append(ResidTempBlock(in_channels, channel_size[i-1], out_channels,\n",
        "                                    kernel_size, stride=stride,\n",
        "                                    dilation=dilation_size))\n",
        "\n",
        "        self.tcn = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # tcn needs data to be (batch_size, input_size, input_length) so we need to swap last 2 dims\n",
        "        x = torch.transpose(x, 1,2)\n",
        "\n",
        "        out = self.tcn(x)\n",
        "\n",
        "        # swap data back so its the correct shape for loss/ etc.\n",
        "        out = torch.transpose(out, 1, 2)\n",
        "        #out = out.view(batch_size, input_length, input_size)\n",
        "        return out\n",
        "\n",
        "    def forward_autoregressive(self, x0, max_len):\n",
        "\n",
        "        all_out = torch.zeros(x0.size(0), max_len+1, x0.size(2)).to(x0.device) # preallocate output\n",
        "        all_out[:,0,:] = x0[:,0,:]\n",
        "        for t in tqdm(range(max_len)):\n",
        "            xt = all_out[:, :t+1, :]\n",
        "            next_output = self.forward(xt)[:,-1,:].unsqueeze(1)\n",
        "            all_out[:,t+1,:] = next_output.squeeze(1)\n",
        "        return all_out[:, 1:, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6UIJmZnCYoo"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def merge_dict(main_dict, new_dict, value_fn = None):\n",
        "    \"\"\"\n",
        "    Merge new_dict into main_dict. If a key exists in both dicts, the values are appended.\n",
        "    Else, the key-value pair is added.\n",
        "    If value_fn is not None, it is applied to each item in each value in new_dict before merging.\n",
        "    Args:\n",
        "        main_dict: main dict\n",
        "        new_dict: new dict\n",
        "        value_fn: function to apply to each item in each value in new_dict before merging\n",
        "    \"\"\"\n",
        "    if value_fn is None:\n",
        "        value_fn = lambda x: x\n",
        "    for key, value in new_dict.items():\n",
        "        if not isinstance(value, list):\n",
        "            value = [value]\n",
        "        value = [value_fn(v) for v in value]\n",
        "        if key in main_dict:\n",
        "            main_dict[key] = main_dict[key] + value\n",
        "        else:\n",
        "            main_dict[key] = value\n",
        "    return main_dict\n",
        "\n",
        "def train(model, optimizer, train_loader, test_loader, criterion, device, num_epochs = 10, reg_coef=0.05):\n",
        "    outputs = {'train_loss': [],\n",
        "               'train_loss_over_time': [],\n",
        "               'test_loss': [],\n",
        "               'test_loss_over_time': [],\n",
        "               'test_loss_autoregressive': [],\n",
        "               'test_loss_autoregressive_over_time': []}\n",
        "    for epoch in range(num_epochs):\n",
        "        #### Training\n",
        "        train_out = train_loop(model, optimizer, train_loader, criterion, device,reg_coef = reg_coef)\n",
        "        outputs = merge_dict(outputs, train_out)\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_out[\"train_loss\"]}')\n",
        "        if epoch == (num_epochs - 1):\n",
        "\n",
        "          #### Eval\n",
        "          test_out = test_loop(model, test_loader, criterion, device)\n",
        "          outputs = merge_dict(outputs, test_out)\n",
        "\n",
        "          print(f'Epoch [{epoch + 1}/{num_epochs}], Test Loss: {test_out[\"test_loss\"]}')\n",
        "          print(f'Epoch [{epoch + 1}/{num_epochs}], Test Loss Autoregressive: {test_out[\"test_loss_autoregressive\"]}')\n",
        "\n",
        "    torch.save(test_out[\"test_preds\"], f'test_preds_{epoch}_no_reg.pt')\n",
        "    torch.save(test_out[\"test_autoregressive_preds\"], f'test_autoregressive_preds_{epoch}_no_reg.pt')\n",
        "    # torch.save(model, 'model_no_reg.pth')\n",
        "\n",
        "    return outputs\n",
        "\n",
        "def random_seed(seed=42, rank=0):\n",
        "    torch.manual_seed(seed + rank)\n",
        "    np.random.seed(seed + rank)\n",
        "    random.seed(seed + rank)\n",
        "\n",
        "def train_loop(model, optimizer, train_loader, criterion, device, reg_coef=0.05):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loss = 0\n",
        "    train_loss_over_time = np.zeros((SEQ_LEN,))\n",
        "    for i, (X_train, Y_train) in enumerate(tqdm(train_loader)):\n",
        "        B, num_chunks, T, d = X_train.shape\n",
        "        X_train = X_train.reshape(B*num_chunks, T, d).to(device)\n",
        "        Y_train = Y_train.reshape(B*num_chunks, T, d).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, Y_train)\n",
        "        train_loss_over_time += torch.mean((outputs - Y_train) ** 2, dim=(0,2)).detach().cpu().numpy()\n",
        "        if reg_coef > 0:\n",
        "            n_atoms = d // 3\n",
        "            outputs = outputs.view(B * num_chunks, T, n_atoms, 3)\n",
        "\n",
        "            # Calculate the centroid for each configuration\n",
        "            centroid = outputs.mean(dim=2, keepdim=True)  # Size: [B*num_chunks, T, 1, 3]\n",
        "\n",
        "            # Calculate the distance of each atom from its centroid\n",
        "            distances = outputs - centroid  # Broadcasting the centroid across all atoms\n",
        "\n",
        "            # Calculate the Euclidean distance\n",
        "            distances = distances.norm(dim=-1)  # Size: [B*num_chunks, T, n_atoms]\n",
        "\n",
        "            # Calculate the average distance to centroid for each configuration\n",
        "            avg_dist_to_centroid = distances.mean(dim=-1)  # Size: [B*num_chunks, T]\n",
        "\n",
        "            # You can then use avg_dist_to_centroid as a regularization term\n",
        "            # For example, add it to your loss function multiplied by the regularization coefficient\n",
        "            reg_loss = reg_coef * avg_dist_to_centroid.mean()\n",
        "\n",
        "            loss += -reg_loss\n",
        "            # print(-reg_loss)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()  # Compute gradient\n",
        "        optimizer.step()  # Update weights\n",
        "    return {'train_loss': train_loss / len(train_loader),\n",
        "            'train_loss_over_time': train_loss_over_time / len(train_loader)}\n",
        "\n",
        "def test_loop(model, test_loader, criterion, device, seq_len = SEQ_LEN):\n",
        "    model.eval()\n",
        "    test_preds = []\n",
        "    test_autoregressive_preds = []\n",
        "    test_loss = 0\n",
        "    test_loss_over_time = np.zeros((seq_len,))\n",
        "    test_loss_autoregressive = 0\n",
        "    test_loss_autoregressive_over_time = np.zeros((seq_len,))\n",
        "    for i, (X_test, Y_test) in enumerate(test_loader):\n",
        "        B, num_chunks, T, d = X_test.shape\n",
        "        X_test = X_test.reshape(B*num_chunks, T, d).to(device)\n",
        "        Y_test = Y_test.reshape(B*num_chunks, T, d).to(device)\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(X_test)\n",
        "        loss = criterion(outputs, Y_test)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Forward pass for autorergressive\n",
        "        with torch.no_grad():\n",
        "            outputs_autoregressive = model.forward_autoregressive(X_test[:,0,:].unsqueeze(1), max_len = X_test.size(1))\n",
        "        loss_autoregressive = criterion(outputs_autoregressive, Y_test)\n",
        "        test_loss_autoregressive += loss_autoregressive.item()\n",
        "\n",
        "        test_preds.append(outputs.detach().cpu().numpy())\n",
        "        test_autoregressive_preds.append(outputs_autoregressive.detach().cpu().numpy())\n",
        "\n",
        "        test_loss_over_time += torch.mean((outputs - Y_test) ** 2, dim=(0,2)).detach().cpu().numpy()\n",
        "        test_loss_autoregressive_over_time += torch.mean((outputs_autoregressive - Y_test) ** 2, dim=(0,2)).detach().cpu().numpy()\n",
        "    return {'test_preds': np.concatenate(test_preds, axis=0),\n",
        "            'test_autoregressive_preds': np.concatenate(test_autoregressive_preds, axis=0),\n",
        "            'test_loss': test_loss / len(test_loader),\n",
        "            'test_loss_over_time': test_loss_over_time / len(test_loader),\n",
        "            'test_loss_autoregressive': test_loss_autoregressive / len(test_loader),\n",
        "            'test_loss_autoregressive_over_time': test_loss_autoregressive_over_time / len(test_loader)}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiSHke2VoWJQ"
      },
      "outputs": [],
      "source": [
        "VALID_MODEL_TYPES = ['RNN', 'Xformer','Transformer','TCN']\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set data parameters\n",
        "train_batch_size = 4\n",
        "test_batch_size = 8\n",
        "num_workers = 2\n",
        "seed = 7\n",
        "\n",
        "\n",
        "\n",
        "# training parameters\n",
        "lr = 1e-4\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "\n",
        "def model_generator(model_type,INPUT_SIZE,OUTPUT_SIZE,SEQ_LEN,params):\n",
        "\n",
        "    # set up default parameters for the models\n",
        "    default_params = {\n",
        "    # rnn parameters\n",
        "    'RNN':{'input_size' :INPUT_SIZE,\n",
        "                            'hidden_size':32,\n",
        "                            'output_size': OUTPUT_SIZE,\n",
        "                            'num_layers':1},\n",
        "\n",
        "\n",
        "    # xformer parameters\n",
        "    'Xformers': {'input_size':INPUT_SIZE,\n",
        "                              'output_size':OUTPUT_SIZE,\n",
        "                              'embed_size': 32,\n",
        "                              'num_layers': 2,\n",
        "                              'num_heads': 8,\n",
        "                              'seq_len':SEQ_LEN},\n",
        "\n",
        "    # transformer parameters\n",
        "    'Transformer':{'input_size' :INPUT_SIZE,\n",
        "                              'output_size' : OUTPUT_SIZE,\n",
        "                              'embed_size' : 96,\n",
        "                              'num_layers' : 6,\n",
        "                              'num_heads' : 16,\n",
        "                              'seq_len' : SEQ_LEN},\n",
        "\n",
        "    #TCN parameters\n",
        "    'TCN':{'input_size' : INPUT_SIZE,\n",
        "                          'channel_size' : [6]*(int(np.ceil(np.log2(((SEQ_LEN-1)*(2-1))/(2*(8-1)) +1 ))) - 2),\n",
        "                          'input_length' : SEQ_LEN,\n",
        "                          'kernel_size' : 8}\n",
        "                     }\n",
        "    if params is not None:\n",
        "        model_params = { **params}\n",
        "    else:\n",
        "        model_params = default_params[model_type]\n",
        "\n",
        "    # Model instantiation\n",
        "    assert model_type in VALID_MODEL_TYPES, f'model_type must be one of {VALID_MODEL_TYPES}'\n",
        "    if model_type == 'RNN':\n",
        "        # Model instantiation\n",
        "            model = RNNModel(**model_params)\n",
        "    elif model_type == 'Xformer':\n",
        "            model = TransformerModel(**model_params)\n",
        "    elif model_type == 'Transformer':\n",
        "            model = TransformerModel_NN(**model_params)\n",
        "    elif model_type == 'TCN':\n",
        "            model = TCNModel(**model_params)\n",
        "    else:\n",
        "        raise ValueError(f'Invalid model_type {model_type}')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuned learning rate"
      ],
      "metadata": {
        "id": "XY-isbpvSdAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate_dict = {\n",
        "    ('RNN', 'small'): 0.0001,\n",
        "    ('RNN', 'medium'): 0.0001,\n",
        "    ('RNN', 'large'): 0.0001,\n",
        "    ('TCN', 'small'): 0.01,\n",
        "    ('TCN', 'medium'): 0.01,\n",
        "    ('TCN', 'large'): 0.001,\n",
        "    ('Xformer', 'small'): 0.01,\n",
        "    ('Xformer', 'medium'): 0.001,\n",
        "    ('Xformer', 'large'): 0.001,\n",
        "    ('Transformer', 'small'): 0.001,\n",
        "    ('Transformer', 'medium'): 0.001,\n",
        "    ('Transformer', 'large'): 0.0001,\n",
        "}"
      ],
      "metadata": {
        "id": "6AsJs9mbSh1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run specific model and save outputs:"
      ],
      "metadata": {
        "id": "Cj6nXUan8_7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed(seed=seed)\n",
        "train_loader, test_loader = build_loaders(train_batch_size=train_batch_size,\n",
        "                                          test_batch_size=test_batch_size,\n",
        "                                          num_workers=num_workers,\n",
        "                                          seed=seed)\n",
        "\n",
        "model_type = 'Transformer'\n",
        "model = model_generator(model_type,INPUT_SIZE,OUTPUT_SIZE,SEQ_LEN,params = None)"
      ],
      "metadata": {
        "id": "3DQVDH4XRymq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "model.to(device)\n",
        "\n",
        "outputs = train(model, optimizer, train_loader, test_loader, criterion, device, num_epochs = epochs,reg_coef=0)"
      ],
      "metadata": {
        "id": "h_rPyXHQ87fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed(seed=seed)\n",
        "train_loader, test_loader = build_loaders(train_batch_size=train_batch_size,\n",
        "                                          test_batch_size=test_batch_size,\n",
        "                                          num_workers=num_workers,\n",
        "                                          seed=seed)"
      ],
      "metadata": {
        "id": "cNzE9X1457I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "min_loss = np.inf\n",
        "for i, (X_test, Y_test) in enumerate(test_loader):\n",
        "\n",
        "  B, num_chunks, T, d = X_test.shape\n",
        "  X_test = X_test.reshape(B*num_chunks, T, d).to(device)\n",
        "  Y_test = Y_test.reshape(B*num_chunks, T, d).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "\n",
        "  loss = nn.functional.mse_loss(Y_test, outputs,reduction = 'none').mean(dim=-1)\n",
        "  min_val, min_idx_cols = torch.min(loss, dim=0)\n",
        "  overall_min_val, min_idx_row = torch.min(min_val, dim=0)\n",
        "  print\n",
        "  if overall_min_val < min_loss:\n",
        "    min_idx = (min_idx_row.item(), min_idx_cols[min_idx_row].item())\n",
        "    sample_test = Y_test[min_idx[1],min_idx[0],:].detach().cpu().numpy()\n",
        "    sample_output = outputs[min_idx[1],min_idx[0],:].detach().cpu().numpy()\n",
        "    min_loss = overall_min_val\n",
        "    print(overall_min_val)\n",
        "\n",
        "\n",
        "  sample_test = sample_test.reshape(-1,3)\n",
        "  sample_output = sample_output.reshape(-1,3)"
      ],
      "metadata": {
        "id": "UwDcftDNZi8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traj0 = md.load_xtc('trajectory-1.xtc', top='100-fs-peptide-400K.pdb')\n",
        "topology = md.load('100-fs-peptide-400K.pdb').topology\n",
        "backbone = traj0.atom_slice(topology.select(\"protein and backbone\"))\n",
        "bb_slice = backbone[0]\n",
        "\n",
        "bb_slice.xyz = sample_test\n",
        "bb_slice.save('TEST_BACKBONE.pdb')\n",
        "\n",
        "bb_slice.xyz = sample_output\n",
        "bb_slice.save('OUTPUT_BACKBONE.pdb')"
      ],
      "metadata": {
        "id": "8PpNgdTGeIqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "min_loss = np.inf\n",
        "\n",
        "for i, (X_train, Y_train) in enumerate(train_loader):\n",
        "\n",
        "  B, num_chunks, T, d = X_train.shape\n",
        "  X_train = X_train.reshape(B*num_chunks, T, d).to(device)\n",
        "  Y_train = Y_train.reshape(B*num_chunks, T, d).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(X_train)\n",
        "\n",
        "  loss = nn.functional.mse_loss(Y_train, outputs,reduction = 'none').mean(dim=-1)\n",
        "  min_val, min_idx_cols = torch.min(loss, dim=0)\n",
        "  overall_min_val, min_idx_row = torch.min(min_val, dim=0)\n",
        "  print\n",
        "  if overall_min_val < min_loss:\n",
        "    min_idx = (min_idx_row.item(), min_idx_cols[min_idx_row].item())\n",
        "    sample_train = Y_train[min_idx[1],min_idx[0],:].detach().cpu().numpy()\n",
        "    sample_output = outputs[min_idx[1],min_idx[0],:].detach().cpu().numpy()\n",
        "    min_loss = overall_min_val\n",
        "    print(overall_min_val)\n",
        "\n",
        "\n",
        "  sample_train = sample_train.reshape(-1,3)\n",
        "  sample_output = sample_output.reshape(-1,3)"
      ],
      "metadata": {
        "id": "CRMhNPeEh1BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape"
      ],
      "metadata": {
        "id": "pjDm0LjE8J9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_train.shape"
      ],
      "metadata": {
        "id": "0pgII1ddlnsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traj0 = md.load_xtc('trajectory-1.xtc', top='100-fs-peptide-400K.pdb')\n",
        "topology = md.load('100-fs-peptide-400K.pdb').topology\n",
        "backbone = traj0.atom_slice(topology.select(\"protein and backbone\"))\n",
        "bb_slice = backbone[0]\n",
        "\n",
        "bb_slice.xyz = sample_train\n",
        "bb_slice.save('TRAIN_BACKBONE.pdb')\n",
        "\n",
        "bb_slice.xyz = sample_output\n",
        "bb_slice.save('TRAIN_OUTPUT_BACKBONE.pdb')"
      ],
      "metadata": {
        "id": "AsAcxxADik0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhx8VzBlCYop"
      },
      "outputs": [],
      "source": [
        "output_dir = Path(f'outputs_{model_type}')\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "stack_keys = ['train_loss_over_time', 'test_loss_over_time', 'test_loss_autoregressive_over_time']\n",
        "for key in stack_keys:\n",
        "    if not isinstance(outputs[key], np.ndarray):\n",
        "        outputs[key] = np.stack(outputs[key])\n",
        "\n",
        "all_train_loss = outputs['train_loss']\n",
        "all_test_loss = outputs['test_loss']\n",
        "all_test_loss_autoregressive = outputs['test_loss_autoregressive']\n",
        "all_train_loss_over_time = outputs['train_loss_over_time']\n",
        "all_test_loss_over_time = outputs['test_loss_over_time']\n",
        "all_test_loss_autoregressive_over_time = outputs['test_loss_autoregressive_over_time']\n",
        "\n",
        "print(f'Final Train Loss: {all_train_loss[-1]}')\n",
        "print(f'Final Test Loss: {all_test_loss[-1]}')\n",
        "print(f'Final Test Loss Autoregressive: {all_test_loss_autoregressive[-1]}')\n",
        "\n",
        "# Plot the train and test loss progression per epoch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(all_train_loss, label='train')\n",
        "# plt.plot(all_test_loss, label='test')\n",
        "plt.plot(all_test_loss_autoregressive, label='test autoregressive')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig(Path(output_dir) / 'loss.png')\n",
        "plt.show()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "    fig.set_size_inches(5, 4)\n",
        "    plt.plot(all_train_loss_over_time[epoch,:], label=f'train {epoch}')\n",
        "    plt.plot(all_test_loss_autoregressive_over_time[epoch,:], label=f'test AR {epoch}')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig(Path(output_dir) / f'loss_epoch{epoch}_ar_only.png')\n",
        "    plt.show()\n",
        "\n",
        "save_pkl(Path(output_dir) / 'outputs.pkl', outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establish model comparison criteria using MACS"
      ],
      "metadata": {
        "id": "2cOlLD9j9F1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to assess computational complexity of model\n",
        "from deepspeed.profiling.flops_profiler import get_model_profile\n",
        "\n",
        "def get_comp(model,batchsize=1, sequence_length = 999, input_size = 264, x=None):\n",
        "    if x is None:\n",
        "          x = torch.rand(batchsize, sequence_length, input_size)\n",
        "    else:\n",
        "          batchsize = x.shape[0]\n",
        "    # model.eval()\n",
        "    flops, macs, params = get_model_profile(model=model, # model\n",
        "                                    input_shape=(batchsize, sequence_length, input_size),  # Adjust the input shape based on your model\n",
        "                                    args=(x,),  # Passing the input as part of args\n",
        "                                    print_profile=False,  # Set to True if you want to print the profile\n",
        "                                    detailed=False,  # Set to True for a detailed profile\n",
        "                                    module_depth=-1,  # Adjust as needed\n",
        "                                    top_modules=1,  # Adjust as needed\n",
        "                                    warm_up=10,  # Number of warm-ups\n",
        "                                    as_string=False,  # Set to True if you want human-readable strings\n",
        "                                    output_file=None,  # Set a file path to save the profile\n",
        "                                    ignore_modules=None  # List any modules to ignore\n",
        "                                    )\n",
        "    return flops, macs, params\n",
        "\n",
        "# Define function that assesses the computational complexity of the model\n",
        "from torchprofile import profile_macs\n",
        "def get_macs(model,batchsize=1, sequence_length = 999, input_size = 264, x=None):\n",
        "    if x is None:\n",
        "          x = torch.rand(batchsize, sequence_length, input_size)\n",
        "    else:\n",
        "          batchsize = x.shape[0]\n",
        "    # model.eval()\n",
        "    macs = profile_macs(model, x)\n",
        "    return macs"
      ],
      "metadata": {
        "id": "U1I-TovOSIYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to fetch the correct parameters based on the chosen model size\n",
        "def get_model_parameters(model_type, size, input_size, output_size, seq_len):\n",
        "    \"\"\"\n",
        "    Get parameters for a specified model type and size.\n",
        "\n",
        "    :param model_type: Type of the model ('RNN', 'Xformer', 'Transformer', 'TCN').\n",
        "    :param size: Size of the model ('small', 'medium', 'large').\n",
        "    :param input_size: Size of the input.\n",
        "    :param output_size: Size of the output.\n",
        "    :param seq_len: Sequence length.\n",
        "    :return: Dictionary of model parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    if model_type == 'RNN':\n",
        "        hidden_sizes = {'small': 256, 'medium': 128*5, 'large': 256*10}\n",
        "        num_layers = {'small': 1, 'medium': 2, 'large': 6}\n",
        "        return {\n",
        "            'input_size': input_size,\n",
        "            'hidden_size': hidden_sizes[size],\n",
        "            'output_size': output_size,\n",
        "            'num_layers': num_layers[size]\n",
        "        }\n",
        "\n",
        "    elif model_type == 'Xformer' or model_type == 'Transformer':\n",
        "        embed_sizes = {'small': 32, 'medium': 64, 'large': 96}\n",
        "        num_layers = {'small': 2, 'medium': 4, 'large': 6}\n",
        "        num_heads = {'small': 4, 'medium': 8, 'large': 16}\n",
        "        return {\n",
        "            'input_size': input_size,\n",
        "            'output_size': output_size,\n",
        "            'embed_size': embed_sizes[size],\n",
        "            'num_layers': num_layers[size],\n",
        "            'num_heads': num_heads[size],\n",
        "            'seq_len': seq_len\n",
        "        }\n",
        "\n",
        "    elif model_type == 'TCN':\n",
        "        channel_sizes = {'small': [8] * max(1,(int(np.ceil(np.log2((seq_len/10 - 1) * (2 - 1) / (2 * (8 - 1)) + 1))) - 2)),\n",
        "                         'medium': [16] * max(1,(int(np.ceil(np.log2((seq_len - 1) * (2 - 1) / (2 * (8 - 1)) + 1))) - 2)),\n",
        "                         'large': [48] * max(1,(int(np.ceil(np.log2((seq_len - 1) * (2 - 1) / (2 * (8 - 1)) + 1))) - 2))}\n",
        "        kernel_size = 8\n",
        "        return {\n",
        "            'input_size': input_size,\n",
        "            'channel_size': channel_sizes[size],\n",
        "            'input_length': seq_len,\n",
        "            'kernel_size': kernel_size\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type. Choose from 'rnn', 'xformer', 'transformer', 'tcn'.\")\n"
      ],
      "metadata": {
        "id": "HEsutid89IWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to output the computational complexity and test throughput for all the model sizes for each model of concern\n",
        "def evaluate_models(input_size, output_size, seq_len, batchsize=64, ntest=100):\n",
        "    model_types = ['RNN', 'Xformer', 'Transformer', 'TCN']\n",
        "    sizes = ['small', 'medium', 'large']\n",
        "    results = []\n",
        "\n",
        "    for model_type in model_types:\n",
        "        for size in sizes:\n",
        "            # Get model parameters\n",
        "            params = get_model_parameters(model_type, size, input_size, output_size, seq_len)\n",
        "\n",
        "            # Create model instance\n",
        "            model = model_generator(model_type,input_size, output_size, seq_len, params).cuda()\n",
        "\n",
        "            # Get FLOPs, MACs, and Parameters\n",
        "            flops, macs, params = get_comp(model, input_size=input_size, sequence_length=seq_len)\n",
        "\n",
        "            # Store results for table\n",
        "            results.append({\n",
        "                'model_type': model_type,\n",
        "                'size': size,\n",
        "                'flops': flops,\n",
        "                'macs': macs,\n",
        "                'parameters': params,\n",
        "            })\n",
        "\n",
        "    # Create DataFrame for table\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "ivAqIW-t9QK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "id": "hm1HGxfBZ4gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comp_complex = evaluate_models(input_size=INPUT_SIZE, output_size=OUTPUT_SIZE, seq_len=SEQ_LEN)\n",
        "comp_complex = comp_complex.sort_values(by='size', ascending=False)"
      ],
      "metadata": {
        "id": "btRZzWUlG0t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate plots for FLOPs, MACs, and Parameters\n",
        "criteria = ['flops', 'macs', 'parameters']\n",
        "for criterion in criteria:\n",
        "    # Pivot the DataFrame to get 'model_type' as index and 'size' as columns\n",
        "    pivot_df = comp_complex.pivot(index='model_type', columns='size', values=criterion)\n",
        "\n",
        "    # Plotting\n",
        "    ax = pivot_df.plot(kind='bar', figsize=(5, 5))\n",
        "    # ax.set_ylabel(criterion.title())\n",
        "    ax.set_ylabel(\"MACS\",fontsize = 14)\n",
        "    ax.set_title(f'{criterion.upper()} for Different Models and Sizes')\n",
        "    ax.set_xlabel('Model Type',fontsize = 14)\n",
        "    plt.xticks(rotation=0)  # Rotate the x-axis labels to show them horizontally\n",
        "\n",
        "    ax.tick_params(axis='y', labelsize=13)  # Font size for y-axis\n",
        "\n",
        "    # Get handles and labels\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_order = [2,1, 0]  # Adjust based on your desired order\n",
        "    ordered_handles = [handles[idx] for idx in new_order]\n",
        "    ordered_labels = [labels[idx] for idx in new_order]\n",
        "\n",
        "    # Create the new legend\n",
        "    ax.legend(ordered_handles, ordered_labels, fontsize=10,title='Size')\n",
        "    # plt.legend(title='Size')\n",
        "    plt.tight_layout()  # Adjust the layout to fit all elements\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FBvDsyvYawMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Comparison: test throughput - DONE"
      ],
      "metadata": {
        "id": "ZHbaj0OemLI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function that fetch the test throughput for the model\n",
        "def speed_test(model, ntest=100, batchsize=1, sequence_length = SEQ_LEN, input_size = INPUT_SIZE, x=None):\n",
        "    if x is None:\n",
        "        x = torch.rand(batchsize, sequence_length, input_size).cuda()\n",
        "    else:\n",
        "        batchsize = x.shape[0]\n",
        "    # model.eval()\n",
        "\n",
        "    # Warmup\n",
        "    for i in range(10):\n",
        "        model(x)\n",
        "\n",
        "    # Test\n",
        "    start = time.time()\n",
        "    for i in range(ntest):\n",
        "        with torch.no_grad():\n",
        "              model.forward_autoregressive(x[:,0,:].unsqueeze(1), max_len = x.size(1))\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    elapse = end - start\n",
        "    speed = batchsize * ntest / elapse\n",
        "    # speed = torch.tensor(speed, device=x.device)\n",
        "    # torch.distributed.broadcast(speed, src=0, async_op=False)\n",
        "    # speed = speed.item()\n",
        "    return speed"
      ],
      "metadata": {
        "id": "G95ajsARmNL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "def fetch_performance_metrics_test_thruput(seed = seed, train_batch_size = train_batch_size, test_batch_size = test_batch_size,\n",
        "                              num_workers = num_workers,device = device,learning_rate_dict = learning_rate_dict, num_epochs = epochs,\n",
        "                              input_size=INPUT_SIZE, output_size=OUTPUT_SIZE, seq_len=SEQ_LEN):\n",
        "    performance_metrics = []\n",
        "\n",
        "    # Initialize a dictionary to hold all results\n",
        "    all_results = {}\n",
        "\n",
        "    model_types = ['RNN', 'Xformer', 'Transformer', 'TCN']\n",
        "    sizes = ['small', 'medium', 'large']\n",
        "\n",
        "    for model_type in model_types:\n",
        "        all_results[model_type] = {}\n",
        "        for size in sizes:\n",
        "            print('Current model is: ',model_type,'and size is: ',size)\n",
        "\n",
        "            file_name = f'{model_type}_{size}_test_thruput.pkl'\n",
        "            file_path = os.path.join('/content/drive/My Drive', file_name)\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    results = pickle.load(f)\n",
        "                    all_results[model_type][size] = results\n",
        "                print(f'{file_path} found')\n",
        "            else:\n",
        "\n",
        "                # Get model parameters\n",
        "                params = get_model_parameters(model_type, size, input_size, output_size, seq_len)\n",
        "\n",
        "                # Create model instance\n",
        "                model = model_generator(model_type,input_size, output_size, seq_len, params).cuda()\n",
        "\n",
        "\n",
        "                # Evaluate accuracy\n",
        "                random_seed(seed=seed)\n",
        "                train_loader, test_loader = build_loaders(train_batch_size=train_batch_size,\n",
        "                                                          test_batch_size=test_batch_size,\n",
        "                                                          num_workers=num_workers,\n",
        "                                                          seed=seed)\n",
        "                # Loss and optimizer\n",
        "                criterion = nn.MSELoss()\n",
        "                optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate_dict[(model_type,size)])\n",
        "                model.to(device)\n",
        "\n",
        "                outputs = train(model, optimizer, train_loader, test_loader, criterion, device, num_epochs = epochs)\n",
        "\n",
        "                # output_dir = Path(f'outputs_{model_type}')\n",
        "                # output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "                stack_keys = ['train_loss_over_time', 'test_loss_over_time', 'test_loss_autoregressive_over_time']\n",
        "                for key in stack_keys:\n",
        "                    if not isinstance(outputs[key], np.ndarray):\n",
        "                        outputs[key] = np.stack(outputs[key])\n",
        "\n",
        "                # Measure throughput time\n",
        "                throughput_time = speed_test(model)\n",
        "\n",
        "                # Organize the outputs\n",
        "                results  = {\n",
        "                    'train_loss': outputs['train_loss'],\n",
        "                    'test_loss': outputs['test_loss'],\n",
        "                    'test_loss_autoregressive': outputs['test_loss_autoregressive'],\n",
        "                    'train_loss_over_time': outputs['train_loss_over_time'],\n",
        "                    'test_loss_over_time': outputs['test_loss_over_time'],\n",
        "                    'test_loss_autoregressive_over_time': outputs['test_loss_autoregressive_over_time'],\n",
        "                    'throughput_time': throughput_time\n",
        "                }\n",
        "                all_results[model_type][size] = results\n",
        "\n",
        "\n",
        "            # Store the results\n",
        "            performance_metrics.append({\n",
        "                'model_type': model_type,\n",
        "                'size': size,\n",
        "                'test_loss_autoregressive': all_results[model_type][size]['test_loss_autoregressive'][-1],\n",
        "                'throughput_time': all_results[model_type][size]['throughput_time']\n",
        "            })\n",
        "\n",
        "            # # Flatten the nested dictionary into a table\n",
        "            # rows_list = []\n",
        "            # for model_type, sizes_dict in all_results.items():\n",
        "            #     for size, metrics in sizes_dict.items():\n",
        "            #         row = {'model_type': model_type, 'size': size}\n",
        "            #         row.update(metrics)\n",
        "            #         rows_list.append(row)\n",
        "\n",
        "            # # Create a DataFrame and save to CSV\n",
        "            # results_df = pd.DataFrame(rows_list)\n",
        "            # results_df.to_csv('/content/drive/My Drive/model_results.csv', index=False)\n",
        "\n",
        "            # # Save the results to a JSON file\n",
        "\n",
        "            with open(file_path, 'wb') as f:\n",
        "                 pickle.dump(results, f)\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "def plot_performance_metrics_test_thruput(performance_metrics,markers,colors):\n",
        "    # Convert to DataFrame for easier plotting\n",
        "    df = pd.DataFrame(performance_metrics)\n",
        "\n",
        "    # Sort the DataFrame by learning_rate to ensure the lines are plotted correctly\n",
        "    df.sort_values('size', inplace=True)\n",
        "\n",
        "    # Create a figure and a set of subplots\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "\n",
        "    ## Iterate over each model type\n",
        "    for model_type in df['model_type'].unique():\n",
        "        model_data = df[df['model_type'] == model_type]\n",
        "        model_data.sort_values('size', inplace=True)\n",
        "\n",
        "        # Use the color for the model type\n",
        "        color = colors.get(model_type, 'black')  # Default color if model type not in dict\n",
        "\n",
        "        # Plot the line for this model type\n",
        "        ax.plot(model_data['throughput_time'], model_data['test_loss_autoregressive'],\n",
        "                label=model_type, linestyle='-', color=color)\n",
        "\n",
        "        # Plot the markers for each size\n",
        "        for size in model_data['size'].unique():\n",
        "            size_data = model_data[model_data['size'] == size]\n",
        "            marker = markers.get(size, 'o')  # Default marker if size not in dict\n",
        "\n",
        "            ax.scatter(size_data['throughput_time'], size_data['test_loss_autoregressive'],\n",
        "                      marker=marker, color=color, s=100)  # Set markersize here with 's' parameter\n",
        "\n",
        "\n",
        "    # Set labels and legend\n",
        "\n",
        "    # Set font size of the x and y axis tick labels\n",
        "    ax.tick_params(axis='x', labelsize=15)  # Font size for x-axis\n",
        "    ax.tick_params(axis='y', labelsize=15)  # Font size for y-axis\n",
        "    ax.set_xlabel('Throughput Speed (Num. Seq./s)', fontsize=20)\n",
        "    ax.set_ylabel('Test Loss', fontsize=20)\n",
        "\n",
        "    # Get handles and labels\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "    # Modify the order of your legend here\n",
        "    # Example: if you want the second item to appear first and the first item second\n",
        "    new_order = [0,3,2, 1]  # Adjust based on your desired order\n",
        "    ordered_handles = [handles[idx] for idx in new_order]\n",
        "    ordered_labels = [labels[idx] for idx in new_order]\n",
        "\n",
        "    # Create the new legend\n",
        "    ax.legend(ordered_handles, ordered_labels, fontsize=20)\n",
        "    # ax.legend(fontsize=20)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RTIPPGCLnJnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Comparison: train data amount - DONE"
      ],
      "metadata": {
        "id": "NFqRRIIxTAau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "# Examine the test autoregressive loss with varying amount of train data available\n",
        "def fetch_performance_metrics_train_data_amount(seed = seed, train_batch_size = train_batch_size, test_batch_size = test_batch_size,\n",
        "                              num_workers = num_workers,device = device,learning_rate_dict = learning_rate_dict, num_epochs = epochs,\n",
        "                              input_size=INPUT_SIZE, output_size=OUTPUT_SIZE, seq_len=SEQ_LEN):\n",
        "    performance_metrics = []\n",
        "\n",
        "    # Initialize a dictionary to hold all results\n",
        "    all_results = {}\n",
        "\n",
        "    model_types = ['RNN', 'Xformer', 'Transformer', 'TCN']\n",
        "    train_drop = [0.9,0.7,0.5,0.3]\n",
        "    size = 'small'\n",
        "\n",
        "    for model_type in model_types:\n",
        "        all_results[model_type] = {}\n",
        "        for percent in train_drop:\n",
        "            print('Current model is: ',model_type,'and train drop amount is: ',percent)\n",
        "\n",
        "            file_name = f'{model_type}_{percent}_train_amt.pkl'\n",
        "            file_path = os.path.join('/content/drive/My Drive', file_name)\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    results = pickle.load(f)\n",
        "                    all_results[model_type][percent] = results\n",
        "                print(f'{file_path} found')\n",
        "            else:\n",
        "\n",
        "                # Get model parameters\n",
        "                params = get_model_parameters(model_type, size, input_size, output_size, seq_len)\n",
        "\n",
        "                # Create model instance\n",
        "                model = model_generator(model_type,input_size, output_size, seq_len, params).cuda()\n",
        "\n",
        "\n",
        "                # Evaluate accuracy\n",
        "                random_seed(seed=seed)\n",
        "                train_loader, test_loader = build_loaders(train_batch_size=train_batch_size,\n",
        "                                                          test_batch_size=test_batch_size,\n",
        "                                                          num_workers=num_workers,\n",
        "                                                          seed=seed,train_drop = percent)\n",
        "                # Loss and optimizer\n",
        "                criterion = nn.MSELoss()\n",
        "                optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate_dict[(model_type,size)])\n",
        "                model.to(device)\n",
        "\n",
        "                outputs = train(model, optimizer, train_loader, test_loader, criterion, device, num_epochs = epochs)\n",
        "\n",
        "                # output_dir = Path(f'outputs_{model_type}')\n",
        "                # output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "                stack_keys = ['train_loss_over_time', 'test_loss_over_time', 'test_loss_autoregressive_over_time']\n",
        "                for key in stack_keys:\n",
        "                    if not isinstance(outputs[key], np.ndarray):\n",
        "                        outputs[key] = np.stack(outputs[key])\n",
        "\n",
        "                # # Measure throughput time\n",
        "                # throughput_time = speed_test(model)\n",
        "\n",
        "                # Organize the outputs\n",
        "                results  = {\n",
        "                    'train_loss': outputs['train_loss'],\n",
        "                    'test_loss': outputs['test_loss'],\n",
        "                    'test_loss_autoregressive': outputs['test_loss_autoregressive'],\n",
        "                    'train_loss_over_time': outputs['train_loss_over_time'],\n",
        "                    'test_loss_over_time': outputs['test_loss_over_time'],\n",
        "                    'test_loss_autoregressive_over_time': outputs['test_loss_autoregressive_over_time']\n",
        "                }\n",
        "                all_results[model_type][percent] = results\n",
        "\n",
        "\n",
        "            # Store the results\n",
        "            performance_metrics.append({\n",
        "                'model_type': model_type,\n",
        "                'train_drop_percent': percent,\n",
        "                'test_loss_autoregressive': all_results[model_type][percent]['test_loss_autoregressive'][-1]\n",
        "            })\n",
        "\n",
        "            with open(file_path, 'wb') as f:\n",
        "                 pickle.dump(results, f)\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "def plot_performance_metrics_train_data_amt(performance_metrics,markers,colors):\n",
        "    # Convert to DataFrame for easier plotting\n",
        "    df = pd.DataFrame(performance_metrics)\n",
        "\n",
        "    df['train_data_remaining'] = 100 - df['train_drop_percent']*100\n",
        "\n",
        "    # Sort the DataFrame by the new 'train_data_remaining' column\n",
        "    df.sort_values('train_data_remaining', inplace=True)\n",
        "\n",
        "    # Create a figure and a set of subplots\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    for model_type in df['model_type'].unique():\n",
        "        model_data = df[df['model_type'] == model_type]\n",
        "        ax.plot(model_data['train_data_remaining'], model_data['test_loss_autoregressive'],\n",
        "                label=model_type, marker=markers[model_type], linestyle='-', markersize=8, color=colors[model_type])\n",
        "\n",
        "    # Set labels and legend\n",
        "    ax.tick_params(axis='x', labelsize=15)\n",
        "    ax.tick_params(axis='y', labelsize=15)\n",
        "    ax.set_xlabel('Train Data Amount (%)', fontsize=20)\n",
        "    ax.set_ylabel('Test Loss', fontsize=20)\n",
        "\n",
        "    # Get handles and labels for the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_order = [0, 3, 2, 1]  # Adjust based on your desired order\n",
        "    ordered_handles = [handles[idx] for idx in new_order]\n",
        "    ordered_labels = [labels[idx] for idx in new_order]\n",
        "\n",
        "    # Create the new legend\n",
        "    ax.legend(ordered_handles, ordered_labels, fontsize=20)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "O5WZrAmPTF7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Comparison: learning rate tuning - DONE"
      ],
      "metadata": {
        "id": "rveebOttUyOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "# Examine the test autoregressive loss with varying amount of train data available\n",
        "def fetch_performance_metrics_lr(seed = seed, train_batch_size = train_batch_size, test_batch_size = test_batch_size,\n",
        "                              num_workers = num_workers,device = device, num_epochs = epochs,\n",
        "                              input_size=INPUT_SIZE, output_size=OUTPUT_SIZE, seq_len=SEQ_LEN):\n",
        "    performance_metrics = []\n",
        "\n",
        "    # Initialize a dictionary to hold all results\n",
        "    all_results = {}\n",
        "\n",
        "    model_types = ['RNN', 'Xformer', 'Transformer', 'TCN']\n",
        "    lrs = [1e-2,1e-3,1e-4,1e-5,1e-6]\n",
        "    size = 'small'\n",
        "\n",
        "    for model_type in model_types:\n",
        "        all_results[model_type] = {}\n",
        "        for lr in lrs:\n",
        "            print('Current model is: ',model_type,'and learning rate is: ',lr)\n",
        "\n",
        "            file_name = f'{size}_{model_type}_{lr}.pkl'\n",
        "            file_path = os.path.join('/content/drive/My Drive', file_name)\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    results = pickle.load(f)\n",
        "                    all_results[model_type][lr] = results\n",
        "                print(f'{file_path} found')\n",
        "            else:\n",
        "\n",
        "                # Get model parameters\n",
        "                params = get_model_parameters(model_type, size, input_size, output_size, seq_len)\n",
        "\n",
        "                # Create model instance\n",
        "                model = model_generator(model_type,input_size, output_size, seq_len, params).cuda()\n",
        "\n",
        "\n",
        "                # Evaluate accuracy\n",
        "                random_seed(seed=seed)\n",
        "                train_loader, test_loader = build_loaders(train_batch_size=train_batch_size,\n",
        "                                                          test_batch_size=test_batch_size,\n",
        "                                                          num_workers=num_workers,\n",
        "                                                          seed=seed)\n",
        "                # Loss and optimizer\n",
        "                criterion = nn.MSELoss()\n",
        "                optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "                model.to(device)\n",
        "\n",
        "                outputs = train(model, optimizer, train_loader, test_loader, criterion, device, num_epochs = epochs)\n",
        "\n",
        "                torch.save(model.state_dict(), f'{model_type}_{lr}.pth')\n",
        "\n",
        "                stack_keys = ['train_loss_over_time', 'test_loss_over_time', 'test_loss_autoregressive_over_time']\n",
        "                for key in stack_keys:\n",
        "                    if not isinstance(outputs[key], np.ndarray):\n",
        "                        outputs[key] = np.stack(outputs[key])\n",
        "\n",
        "                # Organize the outputs\n",
        "                results  = {\n",
        "                    'train_loss': outputs['train_loss'],\n",
        "                    'test_loss': outputs['test_loss'],\n",
        "                    'test_loss_autoregressive': outputs['test_loss_autoregressive'],\n",
        "                    'train_loss_over_time': outputs['train_loss_over_time'],\n",
        "                    'test_loss_over_time': outputs['test_loss_over_time'],\n",
        "                    'test_loss_autoregressive_over_time': outputs['test_loss_autoregressive_over_time']\n",
        "                }\n",
        "                all_results[model_type][lr] = results\n",
        "\n",
        "\n",
        "            # Store the results\n",
        "            performance_metrics.append({\n",
        "                'model_type': model_type,\n",
        "                'learning_rate': lr,\n",
        "                'test_loss_autoregressive': all_results[model_type][lr]['test_loss_autoregressive'][-1]\n",
        "            })\n",
        "\n",
        "            with open(file_path, 'wb') as f:\n",
        "                 pickle.dump(results, f)\n",
        "\n",
        "\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "def plot_performance_metrics_lr(performance_metrics, markers,colors):\n",
        "\n",
        "    # Convert to DataFrame for easier plotting\n",
        "    df = pd.DataFrame(performance_metrics)\n",
        "\n",
        "    # Sort the DataFrame by learning_rate to ensure the lines are plotted correctly\n",
        "    df.sort_values('learning_rate', inplace=True)\n",
        "\n",
        "    # Create a figure and a set of subplots\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Plot lines for each model type and annotate points\n",
        "    for model_type in df['model_type'].unique():\n",
        "        model_data = df[df['model_type'] == model_type]\n",
        "        ax.plot(model_data['learning_rate'], model_data['test_loss_autoregressive'],\n",
        "                label=model_type, marker=markers[model_type], linestyle='-', markersize=8, color=colors[model_type])\n",
        "\n",
        "    # Set labels and legend\n",
        "\n",
        "    # Set font size of the x and y axis tick labels\n",
        "    ax.tick_params(axis='x', labelsize=15)  # Font size for x-axis\n",
        "    ax.tick_params(axis='y', labelsize=15)  # Font size for y-axis\n",
        "    ax.set_xlabel('Learning Rate', fontsize=20)\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_ylabel('Test Loss', fontsize=20)\n",
        "\n",
        "    # Get handles and labels\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "    # Modify the order of your legend here\n",
        "    # Example: if you want the second item to appear first and the first item second\n",
        "    new_order = [2,3,1,0]  # Adjust based on your desired order\n",
        "    ordered_handles = [handles[idx] for idx in new_order]\n",
        "    ordered_labels = [labels[idx] for idx in new_order]\n",
        "\n",
        "    # Create the new legend\n",
        "    ax.legend(ordered_handles, ordered_labels, fontsize=20)\n",
        "    # ax.legend(fontsize=20)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "SUXUFEepU5qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Comparison: test on different seq len"
      ],
      "metadata": {
        "id": "wOs2dGOKXpvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "# Examine the test autoregressive loss with varying amount of train data available\n",
        "def fetch_performance_metrics_test_different_seq_lens(seed = seed, train_batch_size = train_batch_size, test_batch_size = test_batch_size,\n",
        "                              num_workers = num_workers,device = device,learning_rate_dict = learning_rate_dict, num_epochs = epochs,\n",
        "                              input_size=INPUT_SIZE, output_size=OUTPUT_SIZE, seq_len=SEQ_LEN):\n",
        "    performance_metrics = []\n",
        "\n",
        "    # Initialize a dictionary to hold all results\n",
        "    all_results = {}\n",
        "\n",
        "    model_types = ['RNN', 'Xformer', 'Transformer', 'TCN']\n",
        "    new_seq_lens = [3000, 5000,7000,9000]\n",
        "    size = 'small'\n",
        "\n",
        "    for model_type in model_types:\n",
        "        all_results[model_type] = {}\n",
        "        print('Current model is: ', model_type)\n",
        "\n",
        "        file_name = f'{model_type}_test_different_seq_lens.pkl'\n",
        "        file_path = os.path.join('/content/drive/My Drive', file_name)\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'rb') as f:\n",
        "                results = pickle.load(f)\n",
        "                all_results = results\n",
        "            print(f'{file_path} found')\n",
        "        else:\n",
        "\n",
        "            # Get model parameters\n",
        "            params = get_model_parameters(model_type, size, input_size, output_size, seq_len)\n",
        "\n",
        "            # Create model instance\n",
        "            model = model_generator(model_type,input_size, output_size, seq_len, params).cuda()\n",
        "\n",
        "\n",
        "            # Evaluate accuracy\n",
        "            random_seed(seed=seed)\n",
        "            train_loader, test_loader = build_loaders(train_batch_size=train_batch_size,\n",
        "                                                        test_batch_size=test_batch_size,\n",
        "                                                        num_workers=num_workers,\n",
        "                                                        seed=seed)\n",
        "            # Loss and optimizer\n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate_dict[(model_type,size)])\n",
        "            model.to(device)\n",
        "\n",
        "            outputs = train(model, optimizer, train_loader, test_loader, criterion, device, num_epochs = num_epochs)\n",
        "\n",
        "            # output_dir = Path(f'outputs_{model_type}')\n",
        "            # output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "            stack_keys = ['train_loss_over_time', 'test_loss_over_time', 'test_loss_autoregressive_over_time']\n",
        "            for key in stack_keys:\n",
        "                if not isinstance(outputs[key], np.ndarray):\n",
        "                    outputs[key] = np.stack(outputs[key])\n",
        "\n",
        "            # # Measure throughput time\n",
        "            # throughput_time = speed_test(model)\n",
        "\n",
        "            # Organize the outputs\n",
        "            results  = {\n",
        "                'train_loss': outputs['train_loss'],\n",
        "                'test_loss': outputs['test_loss'],\n",
        "                'test_loss_autoregressive': outputs['test_loss_autoregressive'],\n",
        "                'train_loss_over_time': outputs['train_loss_over_time'],\n",
        "                'test_loss_over_time': outputs['test_loss_over_time'],\n",
        "                'test_loss_autoregressive_over_time': outputs['test_loss_autoregressive_over_time']\n",
        "            }\n",
        "            all_results[model_type]['train'] = results\n",
        "\n",
        "            for new_seq_len in new_seq_lens:\n",
        "                test_loader.dataset.set_chunk_size(new_seq_len)\n",
        "                test_loader.dataset.set_mode(\"truncate\")\n",
        "                if model_type in ['Transformer', 'Xformer']:\n",
        "                    model.adjust_pe(new_seq_len)\n",
        "                    model.pos_encoder = model.pos_encoder.cuda()\n",
        "                test_outputs = test_loop(model, test_loader, criterion, device, seq_len=new_seq_len - 1)\n",
        "                results  = {\n",
        "                    'test_preds': test_outputs['test_preds'],\n",
        "                    'test_loss': test_outputs['test_loss'],\n",
        "                    'test_loss_autoregressive': test_outputs['test_loss_autoregressive'],\n",
        "                }\n",
        "                all_results[model_type][new_seq_len] = results\n",
        "                print(len(test_outputs['test_preds']))\n",
        "\n",
        "\n",
        "\n",
        "        # Store the results\n",
        "\n",
        "        performance_metrics.append({\n",
        "            'model_type': model_type,\n",
        "            'seq_len': 1000,\n",
        "            'test_loss_autoregressive': all_results[model_type]['train']['test_loss_autoregressive'][-1],\n",
        "\n",
        "        })\n",
        "\n",
        "\n",
        "        for new_seq_len in new_seq_lens:\n",
        "            print(all_results[model_type][new_seq_len]['test_loss_autoregressive'])\n",
        "            performance_metrics.append({\n",
        "                'model_type': model_type,\n",
        "                'seq_len': new_seq_len,\n",
        "                'test_loss_autoregressive':all_results[model_type][new_seq_len]['test_loss_autoregressive']})\n",
        "\n",
        "        with open(file_path, 'wb') as f:\n",
        "              pickle.dump(all_results, f)\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "def plot_performance_metrics_new_seq_lens(performance_metrics,markers,colors):\n",
        "    # Convert to DataFrame for easier plotting\n",
        "    df = pd.DataFrame(performance_metrics)\n",
        "\n",
        "    # Sort the DataFrame by learning_rate to ensure the lines are plotted correctly\n",
        "    df.sort_values('seq_len', inplace=True)\n",
        "\n",
        "    # Create a figure and a set of subplots\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Plot lines for each model type and annotate points\n",
        "    for model_type in df['model_type'].unique():\n",
        "        model_data = df[df['model_type'] == model_type]\n",
        "        ax.plot(model_data['seq_len'], model_data['test_loss_autoregressive'],\n",
        "                label=model_type, marker=markers[model_type], linestyle='-', markersize=8, color=colors[model_type])\n",
        "\n",
        "    # Set labels and legend\n",
        "\n",
        "    # Set font size of the x and y axis tick labels\n",
        "    ax.tick_params(axis='x', labelsize=15)  # Font size for x-axis\n",
        "    ax.tick_params(axis='y', labelsize=15)  # Font size for y-axis\n",
        "    ax.set_xlabel('Sequence Length', fontsize=20)\n",
        "    ax.set_ylabel('Test Loss', fontsize=20)\n",
        "\n",
        "    # Get handles and labels\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "    # Modify the order of your legend here\n",
        "    # Example: if you want the second item to appear first and the first item second\n",
        "    new_order = [0,3,2,1]  # Adjust based on your desired order\n",
        "    ordered_handles = [handles[idx] for idx in new_order]\n",
        "    ordered_labels = [labels[idx] for idx in new_order]\n",
        "\n",
        "    # Create the new legend\n",
        "    ax.legend(ordered_handles, ordered_labels, fontsize=20)\n",
        "    # ax.legend(fontsize=20)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dnaKS32-X0Z_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "094MANVaMcwh"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 ('cs229')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "3b4f19ea7b7ce707d22da09a8097cdaec93dbdb0b018582a606dcdb2baca45fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}