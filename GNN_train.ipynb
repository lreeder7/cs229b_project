{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69672fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math, random, time\n",
    "import torch, torch_geometric\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import gvp.data\n",
    "from gvp import GVP, GVPConvLayer, LayerNorm\n",
    "\n",
    "import mdtraj as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e72b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structures(files, pdb):\n",
    "    topology = md.load(pdb).topology\n",
    "    start = time.time()\n",
    "    #structures = {}\n",
    "    structures = []\n",
    "    for file in files:\n",
    "        traj = md.load_xtc(file, top=pdb)\n",
    "        traj.center_coordinates()\n",
    "        backbone = traj.atom_slice(topology.select(\"protein and backbone\")) \n",
    "        coords = backbone.xyz # has size (n_frames x n_atoms x 3)\n",
    "        \n",
    "        traj_coords = coords.reshape(-1, backbone.n_atoms*3) # has size (n_frames x n_atoms*3)\n",
    "        for i in range(len(traj)):\n",
    "            structures.append({'coords':traj_coords,\n",
    "                               'name': f'{file[5:-4]}_frame{i}',\n",
    "                               'seq': ['Q']*traj_coords.shape[0]})\n",
    "            #structures[f'{file[5:-4]}_frame{i}'] = {'coords': traj_coords,\n",
    "            #                                        'name': f'{file[5:-4]}_frame{i}',\n",
    "            #                                        'seq': ['Q']*traj_coords.shape[0]} # seq doesn't matter for now\n",
    "    end = time.time()\n",
    "    print(\"create structures took\", round((end-start)/60,4), \"minutes\")\n",
    "    return structures\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3b7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_files(structures):\n",
    "    start = time.time()\n",
    "    random.shuffle(structures)\n",
    "    split = int(0.8*len(structures))\n",
    "    train_struct = structures[0:split]\n",
    "    test_struct = structures[split:]\n",
    "    end = time.time()\n",
    "    print(\"train test split took\", round((end-start)/60,4), \"minutes\")\n",
    "    return train_struct, test_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e90832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_filesSAVED(structures_path, model_dir):\n",
    "    start = time.time()\n",
    "    input_structures = torch.load(structures_path)\n",
    "    random.shuffle(input_structures)\n",
    "    split = int(0.8*len(input_structures))\n",
    "    train_files = input_structures[0:split]\n",
    "    test_files = input_structures[split:]\n",
    "    torch.save(train_files, os.path.join(model_dir,'train_structures.pt'))\n",
    "    torch.save(test_files, os.path.join(model_dir,'test_structures.pt'))\n",
    "    end = time.time()\n",
    "    print(\"Importing structures and splitting took\", round((end-start)/60,4), \"minutes\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cee09ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LigandDataset(gvp.data.ProteinGraphDataset):\n",
    "    def __init_(self, data_list,\n",
    "                num_positional_embeddings=16,\n",
    "                top_k=30, num_rbf=16, device=\"cpu\"):\n",
    "        super(LigandDataset, self).__init__(data_list,\n",
    "                                            num_positional_embeddings,\n",
    "                                            top_k, num_rbf, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e25b5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass LigandDataset(gvp.data.ProteinGraphDataset):\\n    def __init__(self, data_list, \\n                 num_positional_embeddings=16,\\n                 top_k=30, num_rbf=16, device=\"cpu\", node_counts=66,\\n                 tica_dict={}, structures_dict={}):\\n        data.Dataset.__init__(self)\\n        \\n        self.data_list = data_list \\n        self.top_k = top_k\\n        self.num_rbf = num_rbf\\n        self.num_positional_embeddings = num_positional_embeddings\\n        self.device = device\\n        self.node_counts = node_counts\\n        \\n        self.letter_to_num = {\\'C\\': 4, \\'D\\': 3, \\'S\\': 15, \\'Q\\': 5, \\'K\\': 11, \\'I\\': 9,\\n                       \\'P\\': 14, \\'T\\': 16, \\'F\\': 13, \\'A\\': 0, \\'G\\': 7, \\'H\\': 8,\\n                       \\'E\\': 6, \\'L\\': 10, \\'R\\': 1, \\'W\\': 17, \\'V\\': 19, \\n                       \\'N\\': 2, \\'Y\\': 18, \\'M\\': 12}\\n        self.num_to_letter = {v:k for k, v in self.letter_to_num.items()}\\n\\n    def __getitem__(self, i): \\n        return self._featurize_as_graph(self.data_list[i])\\n\\n    def _featurize_as_graph(self, protein):\\n        # overwrite using from_dict and to_dict (on data)\\n        # dict.update to add the state information (as a tensor)\\n        data = super(LigandDataset, self)._featurize_as_graph(protein)\\n        with torch.no_grad():\\n            tica = torch.as_tensor(protein[\\'tica\\'], device=self.device, dtype=torch.float)\\n        data_dict = data.to_dict()\\n        data_dict.update({\\'tica\\':tica})\\n        new_data = torch_geometric.data.Data.from_dict(data_dict)\\n        return new_data\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class LigandDataset(gvp.data.ProteinGraphDataset):\n",
    "    def __init__(self, data_list, \n",
    "                 num_positional_embeddings=16,\n",
    "                 top_k=30, num_rbf=16, device=\"cpu\", node_counts=66,\n",
    "                 tica_dict={}, structures_dict={}):\n",
    "        data.Dataset.__init__(self)\n",
    "        \n",
    "        self.data_list = data_list \n",
    "        self.top_k = top_k\n",
    "        self.num_rbf = num_rbf\n",
    "        self.num_positional_embeddings = num_positional_embeddings\n",
    "        self.device = device\n",
    "        self.node_counts = node_counts\n",
    "        \n",
    "        self.letter_to_num = {'C': 4, 'D': 3, 'S': 15, 'Q': 5, 'K': 11, 'I': 9,\n",
    "                       'P': 14, 'T': 16, 'F': 13, 'A': 0, 'G': 7, 'H': 8,\n",
    "                       'E': 6, 'L': 10, 'R': 1, 'W': 17, 'V': 19, \n",
    "                       'N': 2, 'Y': 18, 'M': 12}\n",
    "        self.num_to_letter = {v:k for k, v in self.letter_to_num.items()}\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        return self._featurize_as_graph(self.data_list[i])\n",
    "\n",
    "    def _featurize_as_graph(self, protein):\n",
    "        # overwrite using from_dict and to_dict (on data)\n",
    "        # dict.update to add the state information (as a tensor)\n",
    "        data = super(LigandDataset, self)._featurize_as_graph(protein)\n",
    "        with torch.no_grad():\n",
    "            tica = torch.as_tensor(protein['tica'], device=self.device, dtype=torch.float)\n",
    "        data_dict = data.to_dict()\n",
    "        data_dict.update({'tica':tica})\n",
    "        new_data = torch_geometric.data.Data.from_dict(data_dict)\n",
    "        return new_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dd1d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCAE(nn.Module):\n",
    "    '''\n",
    "    Graph Convolutional AutoEncoder \n",
    "    \n",
    "    Takes in protein structure graphs of type `torch_geometric.data.Data` \n",
    "    or `torch_geometric.data.Batch` and returns a 3D position per node of \n",
    "    shape [n_nodes, 3].\n",
    "    \n",
    "    Should be used with `gvp.data.ProteinGraphDataset`, or with generators\n",
    "    of `torch_geometric.data.Batch` objects with the same attributes.\n",
    "    \n",
    "    :param node_in_dim: node dimensions in input graph, should be\n",
    "                        (6, 3) if using features from GVP-GNN\n",
    "    :param node_h_dim: hidden node dimensions to use in intermediate layers\n",
    "    :param node_in_dim: edge dimensions in input graph, should be\n",
    "                        (32, 1) if using original features from GVP-GNN\n",
    "    :param edge_h_dim: hidden edge dimensions to embed to before use\n",
    "                       in intermediate layers\n",
    "    :param num_layers: number of layers in each of the encoder\n",
    "                       and decoder modules\n",
    "    :param drop_rate: rate to use in all dropout layers\n",
    "    '''\n",
    "    def __init__(self, node_in_dim, node_h_dim, \n",
    "                 edge_in_dim, edge_h_dim,\n",
    "                 num_layers=3, drop_rate=0.1, node_num=66):\n",
    "    \n",
    "        super(GCAE, self).__init__()\n",
    "        \n",
    "        self.node_num = node_num\n",
    "        \n",
    "        self.W_v = nn.Sequential(\n",
    "            GVP(node_in_dim, node_h_dim, activations=(None, None)),\n",
    "            LayerNorm(node_h_dim)\n",
    "        )\n",
    "        self.W_e = nn.Sequential(\n",
    "            GVP(edge_in_dim, edge_h_dim, activations=(None, None)),\n",
    "            LayerNorm(edge_h_dim)\n",
    "        )\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "                GVPConvLayer(node_h_dim, edge_h_dim, drop_rate=drop_rate) \n",
    "            for _ in range(num_layers))\n",
    "        \n",
    "        \n",
    "        self.squeeze_layer = nn.Sequential(\n",
    "            nn.Linear(self.node_num*(node_h_dim[0]+node_h_dim[1]*3),1024),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Dropout(p=drop_rate),\n",
    "            nn.Linear(1024, 16)\n",
    "        )\n",
    "        \n",
    "        self.unsqueeze_layer = nn.Sequential(\n",
    "            nn.Linear(16, 1024), \n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Dropout(p=drop_rate),\n",
    "            nn.Linear(1024, self.node_num*(node_h_dim[0]+node_h_dim[1]*3))\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "                GVPConvLayer(node_h_dim, edge_h_dim, drop_rate=drop_rate) \n",
    "            for _ in range(num_layers))\n",
    "        \n",
    "        self.W_out = GVP(node_h_dim, (3, 0), activations=(None, None))\n",
    "\n",
    "\n",
    "    def forward(self, h_V, edge_index, h_E):\n",
    "        '''\n",
    "        Forward pass to be used at train-time or test-time.\n",
    "        \n",
    "        :param h_V: tuple (s, V) of node embeddings\n",
    "        :param edge_index: `torch.Tensor` of shape [2, num_edges]\n",
    "        :param h_E: tuple (s, V) of edge embeddings\n",
    "        :param seq: int `torch.Tensor` of shape [num_nodes]\n",
    "        '''\n",
    "        \n",
    "        h_V = (h_V[0].reshape(h_V[0].shape[0]*h_V[0].shape[1],h_V[0].shape[2]),\n",
    "               h_V[1].reshape(h_V[1].shape[0]*h_V[1].shape[1],h_V[1].shape[2],h_V[1].shape[3]))\n",
    "        \n",
    "        h_E = (h_E[0].reshape(h_E[0].shape[0]*h_E[0].shape[1],h_E[0].shape[2]),\n",
    "               h_E[1].reshape(h_E[1].shape[0]*h_E[1].shape[1],h_E[1].shape[2],h_E[1].shape[3]))\n",
    "        \n",
    "        h_V = self.W_v(h_V)\n",
    "        h_E = self.W_e(h_E)\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            h_V = layer(h_V, edge_index, h_E)\n",
    "        \n",
    "        encoder_embeddings = h_V\n",
    "        \n",
    "        flat_s = h_V[0].reshape(h_V[0].shape[0]//self.node_num, -1)\n",
    "        flat_V = h_V[1].reshape(h_V[1].shape[0]//self.node_num, -1)\n",
    "        h_V_stack = torch.cat((flat_s, flat_V),dim=1)\n",
    "        h_V_stack = self.squeeze_layer(h_V_stack)\n",
    "\n",
    "        h_V_small = torch.clone(h_V_stack)\n",
    "        \n",
    "        h_V_stack = self.unsqueeze_layer(h_V_stack)\n",
    "        \n",
    "        flat_s = h_V_stack[:,:self.node_num*encoder_embeddings[0].shape[1]]\n",
    "        flat_V = h_V_stack[:,self.node_num*encoder_embeddings[0].shape[1]:]\n",
    "        h_V = (flat_s.reshape(encoder_embeddings[0].shape), \n",
    "               flat_V.reshape(encoder_embeddings[1].shape))\n",
    "       \n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            h_V = layer(h_V, edge_index, h_E)\n",
    "        logits = self.W_out(h_V)\n",
    "        \n",
    "        logits = logits.reshape(-1, self.node_num, 3)\n",
    "        return logits, h_V_small "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9668fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnn(model_dir, model, train_structures, n_epoch=20):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_dataset = LigandDataset(train_structures)\n",
    "\n",
    "    train_dataloader = torch_geometric.loader.DenseDataLoader(train_dataset, \n",
    "                                                              batch_size=256, \n",
    "                                                              shuffle=True, \n",
    "                                                              num_workers=4)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    losses = torch.zeros(n_epoch,1)\n",
    "    #num_trained = len(glob.glob(os.path.join(model_dir,'epoch-*.pt')))\n",
    "    # if we haven't started training yet, n_epochs will go from 0 -> n_epoch\n",
    "    #n_epochs = list(range(num_trained, num_trained+n_epoch))\n",
    "    n_epochs = list(range(0,n_epoch))\n",
    "    for epoch in n_epochs:\n",
    "        batch_losses = []\n",
    "        start = time.time()\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            nodes = (batch.node_s, batch.node_v)\n",
    "            edges = (batch.edge_s, batch.edge_v)\n",
    "            GT = batch.x\n",
    "            \n",
    "            edge_index = batch.edge_index.permute([1,0,2])\n",
    "            edge_index = edge_index.reshape(2, -1)\n",
    "\n",
    "            pred,latent = model(nodes, edge_index, edges)\n",
    "\n",
    "            # make it so if close in latent space it is close in tica space \n",
    "            # so minimize norm of (pairwise dist in latent minus pairwise dist in tica) \n",
    "            loss = ((GT - pred) ** 2).mean()\n",
    "            \n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #clear_output(wait=True)\n",
    "            #print(f'EPOCH {epoch} BATCH {i} TRAIN loss: {loss:.4f}')\n",
    "        \n",
    "        path = os.path.join(model_dir, 'epoch-{}.pt'.format(epoch))\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # save avg loss value over batch for each epoch\n",
    "        losses[epoch] = np.mean(batch_losses)\n",
    "        torch.save(losses, os.path.join(model_dir, 'losses.pt'))\n",
    "\n",
    "        end = time.time()\n",
    "        t = round((end-start)/60,4)\n",
    "        print(f'----Epoch {epoch} | TRAIN loss: {losses[epoch, 0]} | Elapsed time: {t} minutes----')\n",
    "    \n",
    "    view_output(model_dir, GT, pred, prefix='train_output_')\n",
    "    plot_loss(model_dir, weight)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db35011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, model_dir, test_structures):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #test_structures = torch.load(os.path.join(model_dir,structures))\n",
    "    test_dataset = LigandDataset(test_structures)\n",
    "    test_dataloader = torch_geometric.loader.DenseDataLoader(test_dataset, \n",
    "                                                             batch_size=256, \n",
    "                                                             shuffle=False, \n",
    "                                                             num_workers=4)\n",
    "\n",
    "    model.eval()\n",
    "    latents, preds, GTs = ([] for i in range(3))\n",
    "    losses=[]\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        batch = batch.to(device)\n",
    "        nodes = (batch.node_s, batch.node_v)\n",
    "        edges = (batch.edge_s, batch.edge_v)\n",
    "        GT = batch.x\n",
    "        edge_index = batch.edge_index.permute([1,0,2])\n",
    "        edge_index = edge_index.reshape(2, -1)\n",
    "\n",
    "        pred, latent = model(nodes, edge_index, edges)\n",
    "        \n",
    "        loss = ((GT - pred) ** 2).mean()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "\n",
    "        GTs.extend(GT.cpu().detach().numpy())\n",
    "        latents.extend(latent.cpu().detach().numpy())\n",
    "        preds.extend(logits.cpu().detach().numpy())\n",
    "    print(\"TEST LOSS: \", np.mean(losses))\n",
    "    torch.save(GTs, os.path.join(model_dir, 'inference_GTs.pt'))\n",
    "    torch.save(latents, os.path.join(model_dir, 'inference_latents.pt'))\n",
    "    torch.save(preds, os.path.join(model_dir, 'inference_preds.pt'))\n",
    "    view_output(model_dir, GT, pred, prefix='test_output_')\n",
    "    return latents, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01603a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_output(model_dir, GT, pred, prefix=''):\n",
    "    gt_coords = np.squeeze(GT.cpu().detach().numpy())\n",
    "    pred_coords = np.squeeze(pred.cpu().detach().numpy())\n",
    "    print(pred_coords.shape)\n",
    "    gt_coords = gt_coords.reshape(-1,3)\n",
    "    pred_coords = pred_coords.reshape(-1,3)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(gt_coords[:,0], gt_coords[:,1], gt_coords[:,2])\n",
    "    ax.scatter(pred_coords[:,0], pred_coords[:,1], pred_coords[:,2])\n",
    "    fig.savefig(os.path.join(model_dir,prefix+\"gnn_pred_and_GT.png\"))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(gt_coords[:,0]-pred_coords[:,0], \n",
    "            gt_coords[:,1]-pred_coords[:,1],\n",
    "            gt_coords[:,2]-pred_coords[:,2])\n",
    "    fig.savefig(os.path.join(model_dir,prefix+\"gnn_GT_-_pred.png\"))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0af1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(model_dir, prefix=''):\n",
    "    losses = torch.load(os.path.join(model_dir, prefix+'losses.pt'))\n",
    "    fig, ax = plt.subplots()\n",
    "    loss = ax.plot(range(0,losses.shape[0]), losses,'-.')\n",
    "    fig.title('Loss')\n",
    "    #ax.legend(loss, (f'loss1 + {weight}*loss2', 'loss1', 'loss2'))\n",
    "    print(\"COMBINED LOSS: \", losses)\n",
    "    fig.savefig(os.path.join(model_dir,'loss.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c63ae12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lreeder/miniforge3/envs/pytorch/lib/python3.8/site-packages/mdtraj/formats/pdb/pdbfile.py:200: UserWarning: Unlikely unit cell vectors detected in PDB file likely resulting from a dummy CRYST1 record. Discarding unit cell vectors.\n",
      "  warnings.warn('Unlikely unit cell vectors detected in PDB file likely '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create structures took 0.7065 minutes\n",
      "280000\n"
     ]
    }
   ],
   "source": [
    "files = [f'data/trajectory-{i}.xtc' for i in range(1,29)]\n",
    "pdb = 'data/fs-peptide.pdb'\n",
    "structures = create_structures(files, pdb)\n",
    "\n",
    "print(len(structures))\n",
    "#torch.save(structures, 'gnn_model/input_structures.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66451e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test split took 0.0021 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/lreeder/miniforge3/envs/pytorch/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/lreeder/miniforge3/envs/pytorch/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'LigandDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m edge_in_dim \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#edge dimensions in input graph should be (32, 1) if using original features from GVP-GNN\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m GCAE(node_in_dim, node_h_dim, edge_in_dim, edge_h_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m latents, preds \u001b[38;5;241m=\u001b[39m run_inference(model, model_dir, test_struct)\n",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36mtrain_gnn\u001b[0;34m(model_dir, model, train_structures, n_epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m batch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1027\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_dir = 'gnn_model/'\n",
    "train_struct, test_struct = train_test_split_files(structures)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "node_h_dim = (100, 16)\n",
    "edge_h_dim = (32, 1)\n",
    "node_in_dim = (6, 3) #node dimensions in input graph, should be (6, 3) if using features from GVP-GNN\n",
    "edge_in_dim = (32, 1) #edge dimensions in input graph should be (32, 1) if using original features from GVP-GNN\n",
    "model = GCAE(node_in_dim, node_h_dim, edge_in_dim, edge_h_dim).to(device)\n",
    "\n",
    "model = train_gnn(model_dir, model, train_struct)\n",
    "\n",
    "latents, preds = run_inference(model, model_dir, test_struct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7446660b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
